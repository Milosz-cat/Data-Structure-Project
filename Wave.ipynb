{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c03fa96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import h5py\n",
    "import lmdb\n",
    "import os\n",
    "import time\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2470b9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "\n",
    "def load_images(directory, num_images):\n",
    "    image_paths = []\n",
    "\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith('.png'):  # Change the file extension if necessary\n",
    "                image_paths.append(os.path.join(root, file))\n",
    "\n",
    "    loaded_images = []\n",
    "    count = 0\n",
    "\n",
    "    for image_path in image_paths:\n",
    "        image = cv2.imread(image_path)\n",
    "        loaded_images.append(image)\n",
    "        count += 1\n",
    "\n",
    "        if count >= num_images:\n",
    "            break\n",
    "\n",
    "    return loaded_images\n",
    "\n",
    "# Example usage\n",
    "directory_path = r'C:\\Users\\Miloszek\\Desktop\\wave'  # Replace with your directory path\n",
    "num_images_to_load = 300000  # Specify the number of images to load\n",
    "\n",
    "loaded_images = load_images(directory_path, num_images_to_load)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5575573",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the image\n",
    "cv2.imshow('Image', loaded_images[17])\n",
    "cv2.waitKey(0)  # Wait for a key press to close the window\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7da0d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_directory = r\"C:\\Users\\Miloszek\\Desktop\"\n",
    "\n",
    "#os.mkdir(parent_directory + '\\disk')\n",
    "#os.mkdir(parent_directory + '\\hdf5')\n",
    "#os.mkdir(parent_directory + '\\lmdb')\n",
    "#os.mkdir(parent_directory + '\\_tf')\n",
    "#os.mkdir(parent_directory + '\\_arrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3401bde1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_images(folder_path):\n",
    "    count = 0\n",
    "\n",
    "    for root, dirs, files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            # Check if the file has an image extension\n",
    "            if file.lower().endswith('.png'):\n",
    "                count += 1\n",
    "\n",
    "    return count\n",
    "\n",
    "# Example usage:\n",
    "folder_path = r\"C:\\Users\\Miloszek\\Desktop\\wave\"\n",
    "image_count = count_images(folder_path)\n",
    "print(f\"Total images found: {image_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822c45c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove(folder_path):\n",
    "    for filename in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "\n",
    "        # Check if the current path is a file\n",
    "        if os.path.isfile(file_path):\n",
    "            # Delete the file\n",
    "            os.remove(file_path)\n",
    "            \n",
    "remove(r\"C:\\Users\\Miloszek\\Desktop\\_tf\")\n",
    "remove(r\"C:\\Users\\Miloszek\\Desktop\\disk\")\n",
    "remove(r\"C:\\Users\\Miloszek\\Desktop\\hdf5\")\n",
    "remove(r\"C:\\Users\\Miloszek\\Desktop\\lmdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3bd4437",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "\n",
    "def save_images(images):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    for i, image in enumerate(images):\n",
    "        image_path = os.path.join(r\"C:\\Users\\Miloszek\\Desktop\\disk\", f'image_{i}.png')\n",
    "        cv2.imwrite(image_path, image)\n",
    "    end_time = time.time()\n",
    "\n",
    "    elapsed_time = end_time - start_time\n",
    "    return elapsed_time\n",
    "\n",
    "def save_images_to_hdf5(images):\n",
    "    \n",
    "    images = np.array(images)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    with h5py.File(r\"C:\\Users\\Miloszek\\Desktop\\hdf5\\data.h5\", 'w') as file:\n",
    "        dataset = file.create_dataset('images', data=images)\n",
    "    end_time = time.time()\n",
    "\n",
    "    elapsed_time = end_time - start_time\n",
    "    return elapsed_time\n",
    "\n",
    "\n",
    "def save_images_to_lmdb(images):\n",
    "    \n",
    "    image_size_bytes = images[0].nbytes\n",
    "    map_size = image_size_bytes * len(images) * 10  # Set the map size based on the images size and number of images\n",
    "\n",
    "    env = lmdb.open(r\"C:\\Users\\Miloszek\\Desktop\\lmdb\\data.lmdb\", map_size=map_size)\n",
    "\n",
    "    start_time = time.time()\n",
    "    with env.begin(write=True) as txn:\n",
    "        for i, image in enumerate(images):\n",
    "            key = f\"{i:08}\".encode()\n",
    "            value = image.tobytes()\n",
    "            txn.put(key, value)\n",
    "    end_time = time.time()\n",
    "\n",
    "    elapsed_time = end_time - start_time\n",
    "    return elapsed_time\n",
    "\n",
    "\n",
    "\n",
    "def save_images_to_tfrecord(images):\n",
    "    start_time = time.time()\n",
    "    num_samples = len(images)\n",
    "    tf_writer = tf.io.TFRecordWriter(r\"C:\\Users\\Miloszek\\Desktop\\_tf\\data.tfrecord\")\n",
    "\n",
    "    for i in range(num_samples):\n",
    "        image = images[i]\n",
    "\n",
    "        feature = {\n",
    "            'image': tf.train.Feature(bytes_list=tf.train.BytesList(value=[image.tobytes()])),\n",
    "        }\n",
    "        example = tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "        tf_writer.write(example.SerializeToString())\n",
    "\n",
    "    tf_writer.close()\n",
    "    end_time = time.time()\n",
    "\n",
    "    elapsed_time = end_time - start_time\n",
    "    return elapsed_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29db01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving_time = save_images(loaded_images)\n",
    "#print(\"Time taken to save {} images on disk: {:.3f} seconds\".format(num_images_to_load, saving_time))\n",
    "saving_time = save_images_to_hdf5(loaded_images)\n",
    "print(\"Time taken to save {} images to hdf5: {:.3f} seconds\".format(num_images_to_load, saving_time))\n",
    "saving_time = save_images_to_lmdb(loaded_images)\n",
    "print(\"Time taken to save {} images to lmdb: {:.3f} seconds\".format(num_images_to_load, saving_time))\n",
    "saving_time = save_images_to_tfrecord(loaded_images)\n",
    "print(\"Time taken to save {} images to tfrecord: {:.3f} seconds\".format(num_images_to_load, saving_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42a683a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_images_from_png():\n",
    "    filepaths = r\"C:\\Users\\Miloszek\\Desktop\\disk\"\n",
    "    images = []\n",
    "    start_time = time.time()\n",
    "    for filepath in filepaths:\n",
    "        image = cv2.imread(filepath)\n",
    "        images.append(image)\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    return elapsed_time\n",
    "\n",
    "def read_images_from_hdf5():\n",
    "    filepath = r\"C:\\Users\\Miloszek\\Desktop\\hdf5\\data.h5\"\n",
    "    images = []\n",
    "    start_time = time.time()\n",
    "    with h5py.File(filepath, 'r') as file:\n",
    "        images = file['images']\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    return elapsed_time\n",
    "\n",
    "def read_images_from_lmdb():\n",
    "    filepath = r\"C:\\Users\\Miloszek\\Desktop\\lmdb\\data.lmdb\"\n",
    "    images = []\n",
    "    start_time = time.time()\n",
    "    env = lmdb.open(filepath, readonly=True)\n",
    "    with env.begin() as txn:\n",
    "        cursor = txn.cursor()\n",
    "        for image_id in range(100):\n",
    "            data = txn.get(f\"{image_id:08}\".encode(\"ascii\"))\n",
    "            images.append(data)\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    return elapsed_time\n",
    "\n",
    "def parse_tfrecord_fn(example):\n",
    "    feature_description = {\n",
    "        'image': tf.io.FixedLenFeature([], tf.string),\n",
    "    }\n",
    "    example = tf.io.parse_single_example(example, feature_description)\n",
    "    image = tf.io.decode_raw(example['image'], tf.uint8)\n",
    "    return image\n",
    "\n",
    "def read_images_from_tfrecord():\n",
    "    filepath = r\"C:\\Users\\Miloszek\\Desktop\\_tf\\data.tfrecord\"\n",
    "    start_time = time.time()\n",
    "    dataset = tf.data.TFRecordDataset(filepath)\n",
    "    dataset = dataset.map(parse_tfrecord_fn)\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    return elapsed_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c046f0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "reading_time = read_images_from_png()\n",
    "print(\"Time taken to read {} images from disk: {:.3f} seconds\".format(num_images_to_load, reading_time))\n",
    "reading_time = read_images_from_hdf5()\n",
    "print(\"Time taken to read {} images from hdf5: {:.3f} seconds\".format(num_images_to_load, reading_time))\n",
    "reading_time = read_images_from_lmdb()\n",
    "print(\"Time taken to read {} images from lmdb: {:.3f} seconds\".format(num_images_to_load, reading_time))\n",
    "reading_time = read_images_from_tfrecord()\n",
    "print(\"Time taken to read {} images from tfrecord: {:.3f} seconds\".format(num_images_to_load, reading_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91cbd33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_folder_size(folder_path):\n",
    "    total_size = 0\n",
    "\n",
    "    for path, dirs, files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            file_path = os.path.join(path, file)\n",
    "            total_size += os.path.getsize(file_path)\n",
    "\n",
    "    return total_size\n",
    "\n",
    "size = get_folder_size(r\"C:\\Users\\Miloszek\\Desktop\\disk\")\n",
    "print(f\"DS size: {(size / (1024 * 1024 * 1024)):.2f} GB\")\n",
    "size = get_folder_size(r\"C:\\Users\\Miloszek\\Desktop\\hdf5\")\n",
    "print(f\"DS size: {(size / (1024 * 1024 * 1024)):.2f} GB\")\n",
    "size = get_folder_size(r\"C:\\Users\\Miloszek\\Desktop\\lmdb\")\n",
    "print(f\"DS size: {(size / (1024 * 1024 * 1024)):.2f} GB\")\n",
    "size = get_folder_size(r\"C:\\Users\\Miloszek\\Desktop\\_tf\")\n",
    "print(f\"DS size: {(size / (1024 * 1024 * 1024)):.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44552f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "train_path = r\"C:\\Users\\Miloszek\\Desktop\\wave\\train\"\n",
    "valid_path = r\"C:\\Users\\Miloszek\\Desktop\\wave\\valid\"\n",
    "\n",
    "# Set the image dimensions\n",
    "image_size = (67, 193)  # Adjust to your desired image size\n",
    "\n",
    "# Set batch size\n",
    "batch_size = 32  # Adjust batch size as needed\n",
    "\n",
    "# Load the image dataset from the directory without splitting\n",
    "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    train_path,\n",
    "    image_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    seed=27,\n",
    "    label_mode='binary',\n",
    "    validation_split=0.0  # Set to 0.0 to avoid splitting the dataset\n",
    ")\n",
    "\n",
    "valid_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    valid_path,\n",
    "    image_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    seed=27,\n",
    "    label_mode='binary',\n",
    "    validation_split=0.0  # Set to 0.0 to avoid splitting the dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65901a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import efficientnet.keras as efn\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Initialize EfficientNetB0 model\n",
    "ef = efn.EfficientNetB0(input_shape=(67, 193, 3), weights='imagenet', include_top=False)\n",
    "\n",
    "# Add your own classification layers on top\n",
    "x = ef.output\n",
    "x = GlobalAveragePooling2D()(x)  # Global average pooling to reduce spatial dimensions\n",
    "x = Dense(128, activation='relu')(x)  # Add a fully connected layer with 128 units\n",
    "x = Dense(1, activation='sigmoid')(x)  # Output layer with sigmoid activation for binary classification\n",
    "\n",
    "# Create the final model\n",
    "model = Model(inputs=ef.input, outputs=x)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Print model summary\n",
    "#model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a484117b",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_ds, epochs=3, validation_data=valid_ds)\n",
    "loss, accuracy = model.evaluate(test_ds)\n",
    "print(f'Test loss: {loss:.4f}')\n",
    "print(f'Test accuracy: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a24ff27",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.save_weights('/content/drive/MyDrive/Colab'+' '+'Notebooks/my_model_weights.h5')\n",
    "#model.load_weights('/content/drive/MyDrive/Colab'+' '+'Notebooks/my_model_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554429c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open('/content/drive/MyDrive/Colab'+' '+'Notebooks/224/Clean/Imgclean_765_0.png')\n",
    "\n",
    "# Preprocess the image\n",
    "img_array = np.array(img)\n",
    "img_array = np.expand_dims(img_array, axis=0)\n",
    "\n",
    "# Make a prediction with the model\n",
    "print(model.predict(img_array))\n",
    "prediction = np.argmax(model.predict(img_array))\n",
    "print('Dusty' if prediction == 0 else 'Clean')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
